{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15aabefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import time as t\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c314772a",
   "metadata": {},
   "source": [
    "#### Local time versus UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7251a64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(803075, 49)\n",
      "(803075, 49)\n",
      "~~~~~~~~~~~~~~\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "filtered_edges = np.load( 'CT-allDataFilteredEdges_tropical.npy' )\n",
    "local_time = np.load( 'CT-allDataLocalTimes_tropical.npy' )\n",
    "\n",
    "print( filtered_edges.shape )\n",
    "print( local_time.shape )\n",
    "print( '~~~~~~~~~~~~~~' )\n",
    "times = filtered_edges[:,10]\n",
    "not_multiples_of_three = times[ times%3 != 0 ]\n",
    "print( not_multiples_of_three )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc735cb",
   "metadata": {},
   "source": [
    "#### This cell copies fields from ERA-Interim files to ERA-5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fe2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = [ 'z', 'lifetime' ]\n",
    "for i in np.arange( 1983, 1984):#2009 ):\n",
    "    print( i )\n",
    "    file1 = xr.open_dataset( '/groups/sylvia/JAS-MCS-rain/ERAI/colloc_' + str(i) + '.nc' )\n",
    "    file2 = xr.open_dataset( '/groups/sylvia/JAS-MCS-rain/ERA5/colloc5_' + str(i) + '.nc' )\n",
    "    for v in var:\n",
    "        file2[v] = file1[v]\n",
    "    print(file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21867671",
   "metadata": {},
   "source": [
    "#### This cell extracts only the ISCCP data for a tropical domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a691d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-51.0 51.9\n",
      "-10.0 10.0\n"
     ]
    }
   ],
   "source": [
    "hilat = 10\n",
    "lowlat = -10\n",
    "#ISCCP_data = np.load( \"CT-allDataLocalTimes.npy\" )\n",
    "ISCCP_data = np.load( \"CT-allDataFilteredEdges.npy\" )\n",
    "CT_lat = ISCCP_data[:,12]\n",
    "print( CT_lat.min(), CT_lat.max() )\n",
    "\n",
    "filtered_CT_data = ISCCP_data[(CT_lat >= lowlat) & (CT_lat <= hilat)]\n",
    "#np.save( \"CT-allDataLocalTimes_tropical.npy\", filtered_CT_data )\n",
    "np.save( \"CT-allDataFilteredEdges_tropical.npy\", filtered_CT_data )\n",
    "print( filtered_CT_data[:,12].min(), filtered_CT_data[:,12].max() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c481b0",
   "metadata": {},
   "source": [
    "#### Testing / debugging capeCollocate_ERA5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba74d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.  8.  8. ... 12. 12. 12.]\n"
     ]
    }
   ],
   "source": [
    "filtered_data = np.load( \"CT-allDataLocalTimes_tropical.npy\" )\n",
    "years = filtered_data[:,7]\n",
    "data_1983 = filtered_data[(years == 1983.)]\n",
    "print( data_1983[:,8] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eced12",
   "metadata": {},
   "outputs": [],
   "source": [
    "era_data = xr.open_dataset( '/xdisk/sylvia/ERA5_output/ERA5_qv_tropical.nc' )\n",
    "#era_data = xr.open_dataset( '/xdisk/sylvia/ERA5_output/ERA5_cape_tropical.nc' )\n",
    "#era_data = era_data.assign_coords( longitude=((era_data['longitude'] + 360) % 360) )\n",
    "\n",
    "time = era_data['valid_time'].dt.round(\"S\")\n",
    "longitude = era_data['longitude']\n",
    "display( time )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a618b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in longitude:\n",
    "    print( l.values )\n",
    "    print( l.values%360 )\n",
    "    print( '~~~~~~~~~~~~~~~~~~`' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507dcb89-6311-43cd-afdd-51365c07f3de",
   "metadata": {},
   "source": [
    "#### Look at csv file values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ced2e45f-2543-4e3e-9d3a-7906aee7b9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.007076\n",
      "1            NaN\n",
      "2       0.007436\n",
      "3       0.007021\n",
      "4       0.006983\n",
      "          ...   \n",
      "2868    0.006947\n",
      "2869    0.006802\n",
      "2870    0.007184\n",
      "2871    0.006706\n",
      "2872    0.006745\n",
      "Name: qv_mean, Length: 2873, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "year = 2000\n",
    "df = pd.read_csv( \"output/CT_qv_statistics_\" + str(year) + \"_01.csv\" )\n",
    "print( df['qv_mean'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86389e1b-2ddd-435b-86d4-b40f0969f809",
   "metadata": {},
   "source": [
    "#### Combine csv files one with another AND THEN with MCS precip values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35a4abaf-0860-40dc-91e5-c3efbc412a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2873 2873 2873 2873\n",
      "2694 2694 2694 2694\n",
      "3523 3523 3523 3523\n",
      "4196 4196 4196 4196\n",
      "3849 3849 3849 3849\n",
      "2591 2591 2591 2591\n",
      "2837 2837 2837 2837\n",
      "2544 2544 2544 2544\n",
      "2846 2846 2846 2846\n",
      "3492 3492 3492 3492\n",
      "3272 3272 3272 3272\n",
      "2531 2531 2531 2531\n"
     ]
    }
   ],
   "source": [
    "# Some initial setup and specification of values here\n",
    "basedir1 = '/groups/sylvia/JAS-MCS-rain/ISCCP/output/'\n",
    "#var_list = [ 'qv', 'temperature', 'qc', 'qi', 'w' ]\n",
    "var_list = [ 'temperature', 'qc', 'qi' ]\n",
    "year = 2000\n",
    "for month in np.arange( 1, 13 ):\n",
    "    formatted_month = f\"{month:02d}\"\n",
    "    \n",
    "    filename1 = \"CT_qv_statistics_\" + str(year) + \"_\" + formatted_month + \".csv\"\n",
    "    filename2 = \"CT_temperature_statistics_\" + str(year) + \"_\" + formatted_month + \".csv\"\n",
    "    filename3 = \"CT_qc_statistics_\" + str(year) + \"_\" + formatted_month + \".csv\"\n",
    "    filename4 = \"CT_qi_statistics_\" + str(year) + \"_\" + formatted_month + \".csv\"\n",
    "    #filename5 = \"CT_w_statistics_\" + str(year) + \"_\" + formatted_month + \".csv\"\n",
    "    filename6 = \"CT_cape_statistics_\" + str(year) + \"_\" + formatted_month + \".csv\"\n",
    "    \n",
    "    df1 = pd.read_csv(basedir1 + filename1)\n",
    "    df2 = pd.read_csv(basedir1 + filename2)\n",
    "    df3 = pd.read_csv(basedir1 + filename3)\n",
    "    df4 = pd.read_csv(basedir1 + filename4)\n",
    "    #df5 = pd.read_csv(basedir1 + filename5)\n",
    "    df6 = pd.read_csv(basedir1 + filename6)\n",
    "\n",
    "    df1['temperature_mean'] = df2['temperature_mean']\n",
    "    df1['temperature_99'] = df2['temperature_99']\n",
    "    df1['qc_mean'] = df3['qc_mean']\n",
    "    df1['qc_99'] = df3['qc_99']\n",
    "    df1['qi_mean'] = df4['qi_mean']\n",
    "    df1['qi_99'] = df4['qi_99']\n",
    "    #df1['w_mean'] = df5['w_mean']\n",
    "    #df1['w_99'] = df5['w_99']\n",
    "    df1['cape_mean'] = df6['cape_mean']\n",
    "    df1['cape_99'] = df6['cape_99']\n",
    "\n",
    "    df1.to_csv( \"CT_statistics_\" + str(year) + \"_\" + formatted_month + \".csv\" )\n",
    "    \n",
    "    print( len(df1), len(df2) , len(df3), len(df4) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71853801-a2c1-4dce-8651-2558b4beacf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of yearly reduced dataframe: (17818, 33)\n",
      "17818\n"
     ]
    }
   ],
   "source": [
    "# Some initial setup and specification of values here\n",
    "basedir1 = '/groups/sylvia/JAS-MCS-rain/ISCCP/output/'\n",
    "basedir2 = '/groups/sylvia/JAS-MCS-rain/ERAI/'\n",
    "efile = xr.open_dataset( basedir2 + 'colloc_' + str(year) + '_NZ.nc' )\n",
    "\n",
    "# Initialize an empty dataframe to hold yearly data\n",
    "yearly_reduced_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over months in a given year to combine these in an nc file / yr    \n",
    "m = 0\n",
    "for month in np.arange( 1, 13 ):\n",
    "    formatted_month = f\"{month:02d}\"\n",
    "\n",
    "    # Filter the existing_file for the month of interest\n",
    "    ma = xr.where( efile['month'] == month, 1, 0 )\n",
    "    \n",
    "    # Combine MCS properties to match the csv in the month of interest\n",
    "    num_rows = ma.sum().item()\n",
    "    vars_to_collocate = [ 'day', 'rad', 'ctt', 'lifetime', 'minctt', 'maxrad' ]\n",
    "    ecolloc_mcs_vals = np.full((num_rows, len(vars_to_collocate)), np.nan)  # Initialize with NaN\n",
    "    for i, v in enumerate( vars_to_collocate ):\n",
    "        filtered_vals = efile[v].where(ma == 1, drop=True).values\n",
    "        ecolloc_mcs_vals[:, i] = filtered_vals\n",
    "    \n",
    "    filename = \"CT_statistics_\" + str(year) + \"_\" + formatted_month + \".csv\"\n",
    "    df = pd.read_csv(basedir1 + filename)\n",
    "\n",
    "    # Combine MCS properties from the csv file\n",
    "    ncolloc_mcs_vals = df[['day', 'cs_radius', 'cs_temp', 'lifetime', 'min_temp', 'max_radius']].to_numpy()\n",
    "\n",
    "    # Validate data types before matching\n",
    "    ecolloc_mcs_vals = ecolloc_mcs_vals.astype(np.float64)\n",
    "    ncolloc_mcs_vals = ncolloc_mcs_vals.astype(np.float64)\n",
    "\n",
    "    #print(\"Shape of ecolloc_mcs_vals:\", ecolloc_mcs_vals.shape)\n",
    "    #print(\"Shape of ncolloc_mcs_vals:\", ncolloc_mcs_vals.shape)\n",
    "\n",
    "    # Perform row-wise matching\n",
    "    tolerance = 1e-6\n",
    "    match_matrix = np.all(np.abs(ecolloc_mcs_vals[:, None, :] - ncolloc_mcs_vals[None, :, :]) < tolerance, axis=2)\n",
    "\n",
    "    # Find all matches using np.where\n",
    "    matching_indices = np.array(np.where(match_matrix)).T  # Convert to two-column format\n",
    "\n",
    "    #print(\"Matching indices (ecolloc_mcs_vals, ncolloc_mcs_vals):\")\n",
    "    #print(matching_indices.shape)\n",
    "    m = m + matching_indices.shape[0]\n",
    "\n",
    "    # Extract rows from df at matching_indices[:, 1]\n",
    "    reduced_df = df.iloc[matching_indices[:, 1]].copy()  # Retain only rows at matching indices from df\n",
    "\n",
    "    # Extract the pmax and pacc values at the matching index from ecolloc_mcs_vals\n",
    "    matched_pacc = efile['pacc'].values[matching_indices[:, 0]]  # From ecolloc_mcs_vals\n",
    "    matched_pmax = efile['pmax'].values[matching_indices[:, 0]]  # From ecolloc_mcs_vals\n",
    "\n",
    "    # Add 'pacc' and 'pmax' as new columns to the dataframe df\n",
    "    reduced_df['pacc'] = matched_pacc\n",
    "    reduced_df['pmax'] = matched_pmax\n",
    "    \n",
    "    # Append the reduced_df for the current month to the yearly dataframe\n",
    "    yearly_reduced_df = pd.concat([yearly_reduced_df, reduced_df], ignore_index=True)\n",
    "\n",
    "# Print yearly dataframe shape\n",
    "print(\"Shape of yearly reduced dataframe:\", yearly_reduced_df.shape)\n",
    "\n",
    "# Convert yearly dataframe to xarray Dataset\n",
    "yearly_ds = yearly_reduced_df.to_xarray()\n",
    "\n",
    "# Save the yearly dataset to a NetCDF file\n",
    "output_filename = f\"colloc5_{year}_NZ.nc\"\n",
    "yearly_ds.to_netcdf(basedir1 + output_filename)\n",
    "print( m )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c7d1bb-0f94-4839-a3c7-3ab2e996fd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate new data for this month (occurrence, altitude)\n",
    "    new_occurrences = 10  # Example occurrences per month\n",
    "    altitudes = 5  # Example altitude levels\n",
    "\n",
    "    new_data = xr.Dataset({\n",
    "        \"variable_1D\": ((\"occurrence\"), np.random.rand(new_occurrences)),  # 1D\n",
    "        \"variable_2D\": ((\"occurrence\", \"altitude\"), np.random.rand(new_occurrences, altitudes))  # 2D\n",
    "    },\n",
    "    coords={\n",
    "        \"occurrence\": np.arange(new_occurrences),\n",
    "        \"altitude\": np.arange(altitudes)\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        # Open existing file, read into memory\n",
    "        existing_data = xr.open_dataset(file_path)\n",
    "        \n",
    "        # Concatenate along the occurrence dimension\n",
    "        combined_data = xr.concat([existing_data, new_data], dim=\"occurrence\")\n",
    "\n",
    "        # Save back to NetCDF (overwrite)\n",
    "        combined_data.to_netcdf(file_path, mode=\"w\")  # Mode \"w\" overwrites\n",
    "\n",
    "        print(f\"Appended data for month {month}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, create a new one\n",
    "        new_data.to_netcdf(file_path, mode=\"w\")\n",
    "        print(f\"Created new NetCDF file for month {month}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncplot",
   "language": "python",
   "name": "ncplot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
