{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0df083",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 20240624 The following code is migrated from Paul Vautraver's\n",
    "## RCE_COL_XR_FINISHED.ipynb with updates from Sylvia Sullivan.\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "import random\n",
    "import scipy \n",
    "from dask.distributed import Client, progress, wait\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c658c2",
   "metadata": {},
   "source": [
    "### Client Details\n",
    "Information is called concerning the dask set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca91500a",
   "metadata": {},
   "source": [
    "### Constants\n",
    "Important physical constants are given here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c17e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 9.81\n",
    "MWw = 0.01802 # kg mol-1\n",
    "MWa = 0.02897 # kg mol-1\n",
    "rho_air = 1.3 # kg m-3\n",
    "rho_water = 1000 # kg m-3\n",
    "eps = MWw / MWa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d220a3",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9666a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sst_file_opener(sst):\n",
    "    #Opens a directory of files, each with different time steps, corresponding to a single sea surface temperature (sst).\n",
    "    #Each time step has netcdf data in x,y,z. The output is a joined dataset in time,x,y,z, for a single sst.\n",
    "    \n",
    "    #numerical sst value cast into string\n",
    "    sst_str = str(sst)\n",
    "    \n",
    "    #path created\n",
    "    path = '/xdisk/sylvia/RCE-CAPE-exploration/RCE-sims/ch_cam' + sst_str +'ri0/'\n",
    "    \n",
    "    #dataset created,combining different timesteps in the sst directory\n",
    "    dataset = xr.open_mfdataset(path+'ch_cam'+sst_str+'ri0_4096x64x64_3km_12s_cam'+sst_str+'ri0_64_*.nc',combine='by_coords')\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def precipConversion(qp):\n",
    "    #Function to convert QP to precipitation intensity in units of mm per hour\n",
    "    \n",
    "    rho_air = 1.3  # [kg m-3 air] Technically this should account for altitude dependence\n",
    "    rho_liq = 1000 # [kg m-3 liq water / rain]\n",
    "    del_t   = 0.5  # [h] Temporal resolution of the RCE sims\n",
    "    del_x   = 3000 # [m] Spatial resolution of the RCE sims\n",
    "\n",
    "    # Calculate rain rate [m h-1] and return a value in [m h-1]\n",
    "    # Why do I multiply by del_x**2 rather than dividing? \n",
    "    # Assume 1 m3 of air and 1 m3 of rain. The rain is then distributed over a grid cell of area del_x**2 at some rate.\n",
    "    rr = qp * rho_air / rho_liq / del_t * del_x**2\n",
    "    \n",
    "    # Final factor of 1000 here converst [m h-1] to [mm h-1]\n",
    "    return rr / 1000\n",
    "\n",
    "def storm_labeller(binary_t_xr):\n",
    "    #Function that takes in a binary structure in 3d (time,x and y) and which finds spatially connected\n",
    "    #components in each time step, i.e. identifies clusters without using temporal connectivity\n",
    "    \n",
    "    #Connectivity employed is 4 connectivity\n",
    "    \n",
    "    #creating connected components structure for ndimage\n",
    "    #no temporal connection between clusters, so empty 0th and 2nd array\n",
    "    struct = np.zeros((3,3,3))\n",
    "\n",
    "    #simple 4 connection within current time set\n",
    "    struct[1] = [[0,1,0],[1,1,1],[0,1,0]]\n",
    "\n",
    "    #labels_0 is array of clusters with unique numerical labels, dim=(time,x,y)\n",
    "    #nb_0 is number of identified clusters\n",
    "    labels_0,nb_0 = ndimage.label(binary_t_xr,structure=struct)\n",
    "\n",
    "    #find the sizes of each object, in terms of pixels , dim=(number of connected elements found)\n",
    "    pixelsizes = ndimage.sum(binary_t_xr,labels_0,range(nb_0+1))\n",
    "\n",
    "    #create a boolean array for whether the objects \n",
    "    #are larger than a given threshold\n",
    "    #the threshold was set to 310 by Paul but I think it should be 706 for an eq diameter of 90 km\n",
    "    mask_size = pixelsizes < 705\n",
    "    updated_pixelsizes = pixelsizes[pixelsizes >= 705]\n",
    "    updated_pixelsizes = xr.DataArray( updated_pixelsizes ).rename( 'cluster_sizes' )\n",
    "\n",
    "    #indices of pixels to be removed are produced\n",
    "    #remove_pixel,dim=(time,x,y)\n",
    "    remove_pixel = mask_size[labels_0]\n",
    "  \n",
    "    #indices and thus objects are removed based on object size\n",
    "    labels_0[remove_pixel] = 0\n",
    "    \n",
    "    labels_0 = xr.where(labels_0==0,np.nan,labels_0)\n",
    "    \n",
    "    #cluster array cast into xr array\n",
    "    labelled_xr = xr.DataArray(labels_0,\n",
    "                               dims=['time','y','x'],\n",
    "                               coords={'time':binary_t_xr.time,'y':binary_t_xr.y,\n",
    "                                       'x':binary_t_xr.x})\n",
    "    \n",
    "    return labelled_xr.rename('clusters'), updated_pixelsizes\n",
    "\n",
    "def buoy_xr(x_arr,mcs_xr):\n",
    "    #function to calculate the buoyancy field, outputs x,y,z array per time array\n",
    "    \n",
    "    #environmental temperature,qv and qn are called\n",
    "    #filter out the cells where we have MCSs\n",
    "    tenv = x_arr.TABS * xr.where( mcs_xr.isnull(), 1, np.nan )\n",
    "    tenv_xr = tenv.mean(dim=['x','y'],skipna=True)\n",
    "\n",
    "    qvenv = x_arr.QV * xr.where( mcs_xr.isnull(), 1, np.nan )\n",
    "    qvenv_xr = qvenv.mean(dim=['x','y'])\n",
    "\n",
    "    qnenv = x_arr.QN * xr.where( mcs_xr.isnull(), 1, np.nan )\n",
    "    qnenv_xr = qnenv.mean(dim=['x','y'])\n",
    "    \n",
    "    #buoyancy field calculated\n",
    "    buoyancy_xr = g*(x_arr.TABS - tenv_xr)/tenv_xr + (MWw/MWa)*(qvenv_xr - x_arr.QV) - (qnenv_xr - x_arr.QN)\n",
    "\n",
    "    #filtered for negative buoyancy\n",
    "    buoyancy_xr = xr.where(buoyancy_xr<0,0,buoyancy_xr)\n",
    "    \n",
    "    return buoyancy_xr.rename('buoyancy')\n",
    "\n",
    "def svpl_xr(t_arr):\n",
    "    #function to calculate the saturation vapor pressure over liquid water as a function of temperature\n",
    "    \n",
    "    R = 8.314             # J mol-1 K-1\n",
    "    MWw = 18.015/1000     # kg mol-1\n",
    "    rhoa = 1.395\n",
    "    a1 = 54.842763\n",
    "    a2 = -6763.22\n",
    "    a3 = -4.21\n",
    "    a4 = 0.000367\n",
    "    a5 = 0.0415\n",
    "    a6 = 218.8\n",
    "    a7 = 53.878\n",
    "    a8 = -1331.22\n",
    "    a9 = -9.44523\n",
    "    a10 = 0.014025\n",
    "    \n",
    "    factor = a7 + a8/t_arr + a9*xr.ufuncs.log(t_arr) + a10*t_arr\n",
    "    \n",
    "    psatL = a1 + a2/t_arr + a3*xr.ufuncs.log(t_arr) + a4*t_arr + xr.ufuncs.arctan(a5*(t_arr - a6))*factor\n",
    "    \n",
    "    psatL = xr.ufuncs.exp(psatL)\n",
    "    \n",
    "    return psatL\n",
    "\n",
    "def sd_xr(xr_arr):\n",
    "    #function to calculate the saturation deficit\n",
    "    \n",
    "    return (eps*svpl_xr(xr_arr.TABS))/(xr_arr.p*100 - svpl_xr(xr_arr.TABS))*1000 - xr_arr.QV\n",
    "\n",
    "def sd_masterfunc(dataset,p1,p2,p3):\n",
    "    #function to calculate the lower-tropospheric saturation deficit\n",
    "    #as the average of values at three levels (generally 850, 750, and 500 hPa, following Singh et al. 2017)\n",
    "    \n",
    "    ds_1 = dataset.where(dataset.p>p1,drop=True).isel(z=-1)\n",
    "\n",
    "    ds_2 = dataset.where(dataset.p>p2,drop=True).isel(z=-1)\n",
    "\n",
    "    ds_3 = dataset.where(dataset.p>p3,drop=True).isel(z=-1)\n",
    "    \n",
    "    #sd_mean created and given an xr name\n",
    "    sd_mean = ((sd_xr(ds_1)+sd_xr(ds_2)+sd_xr(ds_3))/3).rename('SD')\n",
    "    \n",
    "    return sd_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f591af",
   "metadata": {},
   "source": [
    "### Super Function\n",
    "Now all of the established functions are used within the final data collecting function, where the fields are produced, including the cluster field, and they are combined into a single xr array output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f404b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_xr(ds_xr):\n",
    "    #super function that uses all of the above functions, as well as simple one line xr routines\n",
    "    #to compile xy fields for each variable, at each timestep. Omega, the pressure velocity, is \n",
    "    #also calculated and stored with z dimensions as well.\n",
    "    \n",
    "    #ds_xr is a dataset in xr array format, naturally assuming all variables called below\n",
    "    \n",
    "    #Surface precipitation values filtered\n",
    "    z = ds_xr.z\n",
    "    psurf = ds_xr.QP.isel(z=0).rename('psurf')\n",
    "    \n",
    "    #precipitation rate found using conversion\n",
    "    p_rate = precipConversion(psurf).rename('p_rate')\n",
    "    \n",
    "    #condensate\n",
    "    #filtered for temperatures below 245K\n",
    "    qn_245 = ds_xr.QN.where(ds_xr.TABS<245)\n",
    "    \n",
    "    #moisture array backfilled along z and lowest array is selected, i.e. first values\n",
    "    #rising in z are selected\n",
    "    qn_245 = qn_245.bfill('z').isel(z=0).rename('qn245')\n",
    "    \n",
    "    #filtered against threshold moisture content\n",
    "    #changed the threshold from 10**(-4)\n",
    "    mcs_xr = xr.where(qn_245<10**(-4),0,1).rename('binary')\n",
    "    \n",
    "    #should take into account changing air density etc\n",
    "    #but constant factor is good first approximation\n",
    "    #the following cancel: divide QV by 1000 to convert g kg-1 -> kg kg-1, multiply cwvc by 1000 to convert m -> mm\n",
    "    cwvc = (ds_xr.QV.integrate('z')*(1.3/1000)).rename('cwvc')\n",
    "    \n",
    "    #calculating the cwvc at saturation to evaluate the CSF\n",
    "    #saturation mixing ratio [g kg-1]\n",
    "    qvsat = (eps*svpl_xr(ds_xr.TABS))/(ds_xr.p*100 - svpl_xr(ds_xr.TABS))*1000\n",
    "    #as above for cwvc: divide QV by 1000 to convert g kg-1 -> kg kg-1, multiply cwvc by 1000 to convert m -> mm\n",
    "    cwvc_sat = (qvsat.integrate('z')*(1.3/1000))\n",
    "    csf = (cwvc / cwvc_sat).rename('csf')\n",
    "    \n",
    "    #calculating csf in different layers\n",
    "    #lowest layer from 1007.7 to 953.44 hPa // layer from 937.32 to 867.26 hPa // layer from 833.72 to 713.09 hPa\n",
    "    #layer from 671.36 to 521.89 hPa\n",
    "    low_indx = [0, 6, 10, 14]\n",
    "    up_indx = [6, 10, 14, 19]\n",
    "    tag_name = ['_BL', '_TROP1', '_TROP2', '_TROP3']\n",
    "    for l, u, k, tt in zip(low_indx, up_indx, np.arange(4), tag_name):\n",
    "        ds_xr['qv'+tt] = ds_xr.QV[:,l:u].rename({'z':'z'+tt})\n",
    "        ds_xr['qvsat'+tt] = qvsat[:,l:u].rename({'z':'z'+tt})\n",
    "        \n",
    "        cwvc_layer = (ds_xr['qv'+tt].integrate('z'+tt)*(1.3/1000))\n",
    "        cwvc_sat_layer = (ds_xr['qvsat'+tt].integrate('z'+tt)*(1.3/1000))\n",
    "        ds_xr['csf'+tt] = (cwvc_layer / cwvc_sat_layer)\n",
    "    \n",
    "    #pressure velocity found\n",
    "    #Sylvia adding ascent and descent rate below\n",
    "    omega = (-(ds_xr.W)*g*rho_air).rename('omega')\n",
    "    ds_xr['ascent'] = xr.where( omega<0, -1.*omega, np.nan )\n",
    "    ds_xr['descent'] = xr.where( omega>0, omega, np.nan )\n",
    "    \n",
    "    #saturation deficit found\n",
    "    sd_mean = sd_masterfunc(ds_xr,550,700,850)\n",
    "    \n",
    "    #slowest step, connected components over 400*64*4096\n",
    "    #no temporal connectivity\n",
    "    #clustering using 4-connectivity, this step finds our mesoscale storms\n",
    "    clusters, cluster_sizes = storm_labeller(mcs_xr) # clusters [=] (400 times, 64 lats, 4096 lons) as other fields\n",
    "    cluster_sizes = cluster_sizes.rename( {'dim_0':'clusters'} )\n",
    "    \n",
    "    #buoyancy calculated and made into a new xr array\n",
    "    #buoyancy_xr = buoy_xr(ds_xr)\n",
    "    #Paul's calculation above. Really, we want to take buoyancy relative to an environment that excludes MCSs\n",
    "    buoyancy_xr = buoy_xr(ds_xr,clusters)\n",
    "    \n",
    "    #cape array made by integrating over z\n",
    "    #cape = buoyancy_xr.integrate('z').rename('CAPE')\n",
    "    #Paul's calculation above. Really, we want only to integrate from cloud base to cloud top, assume FAT - ztop = 200K\n",
    "    tmean = ds_xr.TABS.mean(dim=['time','x','y']).values\n",
    "    i = np.argwhere( (tmean[:-1]-tmean[1:]) < 0 )[0,0]\n",
    "    buoyancy_xr = buoyancy_xr[:,:i].rename({'z':'zTROP'})\n",
    "    cape = buoyancy_xr.integrate('zTROP').rename('CAPE')\n",
    "    \n",
    "    #new environmental arrays are combined into a final xr array\n",
    "    #3D temperature and moisture structure in the storm retained to find precip efficiency\n",
    "    measurements_xr = xr.combine_nested([ p_rate, cape, cwvc, csf, ds_xr.csf_BL, ds_xr.csf_TROP1,\n",
    "                                          ds_xr.csf_TROP2, ds_xr.csf_TROP3, sd_mean, clusters, omega,\n",
    "                                          ds_xr.ascent, ds_xr.descent, ds_xr.TABS, ds_xr.QV, ds_xr.p, ds_xr.QN ],\n",
    "                                        concat_dim=[None], coords='minimal',\n",
    "                                        compat='override' )\n",
    "    \n",
    "    return measurements_xr, cluster_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ddd89d",
   "metadata": {},
   "source": [
    "## Execution 1\n",
    "Extract the mean conditions (SD, CAPE, etc.) for grid cells in the MCS where its precipitation = mean or 99th percentile value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def mask_and_average( data, threshold ):\n",
    "    # Mask data where 'pmax' is greater or equal to the threshold and return the masked data\n",
    "    ds1 = data.where( data['p_rate'] >= threshold, drop=True )\n",
    "    return ds1.mean( dim='stacked_time_y_x' )\n",
    "\n",
    "def _main_execution1_(sst):\n",
    "    #rce data opened, xr arrays generated, then environmental data is collocated against the\n",
    "    #clusters using groupby. Mean and Max \n",
    "    basedir = '/xdisk/sylvia/RCE-CAPE-exploration/'\n",
    "    \n",
    "    #open dataset corresponding to a given sea surface temperature\n",
    "    ds = sst_file_opener(sst)\n",
    "    \n",
    "    #All environmental variable fields and clusters are collected for time,x,y,z\n",
    "    output_xr, cluster_sizes_xr = super_xr(ds)\n",
    "    #display(output_xr)\n",
    "    \n",
    "    #fields are collocated against clusters\n",
    "    output_col = output_xr.groupby( 'clusters' )\n",
    "    print('cluster sizes saved')\n",
    "    cluster_sizes_xr.to_netcdf( basedir + 'clusters_' + str(sst) + '.nc' )\n",
    "    \n",
    "    #identify where the 99th percentile in rainfall is, p_rate_99 [=] clusters\n",
    "    p_rate_99 = output_col.map( lambda group: group['p_rate'].quantile( 0.99, skipna=True ) )\n",
    "\n",
    "    #then average over values in a cluster where p_rate > the 99th percentile\n",
    "    selected_measurements = output_col.map( lambda group: mask_and_average( group, p_rate_99.sel( clusters=group.clusters ) ))\n",
    "    print( '99 calculated' )\n",
    "    file_path = basedir + 'RCE_COL_99_' + str(sst) + '.nc'\n",
    "    selected_measurements.to_netcdf( file_path )\n",
    "    print( '99 written' )\n",
    "    \n",
    "    #identify where the mean rainfall is, p_rate_mean [=] clusters\n",
    "    p_rate_mean = output_col.map( lambda group: group['p_rate'].mean( skipna=True ) )\n",
    "\n",
    "    #then average over values in a cluster where p_rate > the 99th percentile\n",
    "    selected_measurements = output_col.map( lambda group: mask_and_average( group, p_rate_mean.sel( clusters=group.clusters ) ))\n",
    "    print( 'mean calculated' )\n",
    "    file_path = basedir + 'RCE_COL_MEAN_' + str(sst) + '.nc'\n",
    "    selected_measurements.to_netcdf( file_path )\n",
    "    print( 'mean written' )\n",
    "    return\n",
    "\n",
    "_main_execution1_(280)\n",
    "_main_execution1_(285)\n",
    "_main_execution1_(290)\n",
    "_main_execution1_(295)\n",
    "_main_execution1_(300)\n",
    "_main_execution1_(305)\n",
    "_main_execution1_(310)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8102766",
   "metadata": {},
   "source": [
    "## Execution 2\n",
    "Extract the mean or 99th percentile conditions (SD, CAPE, etc.) over ALL grid cells in the MCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c41f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def _main_execution2_(sst):\n",
    "    #rce data opened, xr arrays generated, then environmental data is collocated against the\n",
    "    #clusters using groupby. Mean and Max \n",
    "    basedir = '/xdisk/sylvia/RCE-CAPE-exploration/all-var-percentile/'\n",
    "    \n",
    "    #open dataset corresponding to a given sea surface temperature\n",
    "    ds = sst_file_opener(sst)\n",
    "    \n",
    "    #All environmental variable fields and clusters are collected for time,x,y,z\n",
    "    output_xr, cluster_sizes_xr = super_xr(ds)\n",
    "    \n",
    "    #fields are collocated against clusters\n",
    "    output_col = output_xr.groupby('clusters')\n",
    "    cluster_sizes_xr.to_netcdf( basedir + 'clusters_' + str(sst) + '.nc' )\n",
    "    \n",
    "    #cluster groups are aggregated by taking the 99th percentile value of the clusters\n",
    "    output_99 = output_col.quantile( 0.99, skipna=True )\n",
    "    print( '99 calculated' )\n",
    "    \n",
    "    #99th percentile collocated variables are saved\n",
    "    output_99.to_netcdf( basedir + 'RCE_COL_99_' + str(sst) +'.nc' )\n",
    "    print( '99 written' )\n",
    "    \n",
    "    #cluster groups are aggregated by taking max values of the clusters\n",
    "    output_max = output_col.max()\n",
    "    print( 'max calculated' )\n",
    "    \n",
    "    #max collocated variables are saved\n",
    "    output_max.to_netcdf( basedir + 'RCE_COL_MAX_' + str(sst) +'.nc' )\n",
    "    print( 'max written' )\n",
    "    \n",
    "    #cluster groups are aggregated by taking the 95th percentile value of the clusters\n",
    "    output_95 = output_col.quantile( 0.95, skipna=True )\n",
    "    print( '95 calculated' )\n",
    "    \n",
    "    #95th percentile collocated variables are saved\n",
    "    output_95.to_netcdf( basedir + 'RCE_COL_95_' + str(sst) +'.nc' )\n",
    "    print( '95 written' )\n",
    "    \n",
    "    #cluster groups are aggregated by taking mean values of the clusters\n",
    "    output_mean = output_col.mean()\n",
    "    print( 'mean calculated' )\n",
    "    \n",
    "    #mean collocated variables are saved\n",
    "    output_mean.to_netcdf( basedir + 'RCE_COL_MEAN_' + str(sst) +'.nc' )\n",
    "    print( 'mean written' )\n",
    "    \n",
    "    return\n",
    "\n",
    "_main_execution2_(280)\n",
    "_main_execution2_(285)\n",
    "_main_execution2_(290)\n",
    "_main_execution2_(295)\n",
    "_main_execution2_(300)\n",
    "_main_execution2_(305)\n",
    "_main_execution2_(310)\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncplot",
   "language": "python",
   "name": "ncplot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
