{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy import ndimage\n",
    "import statistics as stat\n",
    "import random\n",
    "import scipy \n",
    "#from matplotlib.colors import BoundaryNorm\n",
    "#from matplotlib.ticker import MaxNLocator\n",
    "import copy\n",
    "from scipy import integrate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "\n",
    "g = 9.8\n",
    "MWa = 0.02897 # kg mol-1\n",
    "MWw = 0.01802 #kg mol-1\n",
    "#for relative humidity and saturation deficit\n",
    "eps = 0.01802/0.02897\n",
    "rho = 1000 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def nan_array(shape):\n",
    "    #function to create arrays filled with np nan values\n",
    "\n",
    "    #empty array\n",
    "    an_array = np.empty(shape)\n",
    "\n",
    "    #iterate thhrough array setting to nans\n",
    "    an_array[:] = np.NaN\n",
    "\n",
    "    return an_array\n",
    "\n",
    "\n",
    "def place_counter(asc_array,N=1):\n",
    "    #function to find number of places taken up by values, eg 1000\n",
    "    #occupies 4 spaces, 11358 takes 5.\n",
    "    #asc_array, an array where values only increase \n",
    "    #N = number of places of first value\n",
    "\n",
    "    n_places = np.array([],dtype=int)\n",
    "\n",
    "    n = N\n",
    "   \n",
    "    for val in asc_array:\n",
    "\n",
    "        while val//(10**n) >= 1:\n",
    "        \n",
    "            n += 1\n",
    "        \n",
    "        #fill list with number of places\n",
    "        n_places = np.append(n_places,int(n))\n",
    "        \n",
    "    return n_places\n",
    "    \n",
    "def file_suffix_maker(zero_number,lower,upper,step):\n",
    "    #many file suffixes feature strings with adjacent 0s, like '00000'\n",
    "    #these are hard to make simply, and this function is required to make them\n",
    "    \n",
    "    zero_str_arr = []\n",
    "    \n",
    "    array = np.arange(lower,upper,step)\n",
    "    \n",
    "    #find number of places,set to start at 4 as files in this notebook start with time 1800\n",
    "    places = place_counter(array,N=4)\n",
    "    \n",
    "    for i,val in enumerate(array):\n",
    "        \n",
    "        #number of 0s required\n",
    "        zero_number_new = zero_number - places[i]\n",
    "        \n",
    "        #creates string with prefix of 0s\n",
    "        zero_str = str(0)*zero_number_new + str(val)\n",
    "        \n",
    "        zero_str_arr = np.append(zero_str_arr,zero_str)\n",
    "        \n",
    "    return zero_str_arr\n",
    "\n",
    "#file_suffixes = file_suffix_maker(10,1800,721800,1800)\n",
    "\n",
    "def file_concatenator(sst,zero_number,lower,upper,step):\n",
    "    #Takes a list of numbers corresponding to filenumbers/years \n",
    "    #and compiles the corresponding list of filenames\n",
    "    \n",
    "    file_names = []\n",
    "    \n",
    "    #base directory where the desired files are located\n",
    "    basedir = '/work/bb1018/b380873/RCE-CAPE-exploration/RCE-sims/ch_cam' + str(sst) + 'ri0/'\n",
    "    \n",
    "    file_suffixes = file_suffix_maker(zero_number,lower,upper,step)\n",
    "    \n",
    "    #iterates through numbers\n",
    "    for suffix in file_suffixes:\n",
    "        \n",
    "        #appending list of files\n",
    "        file_names = np.append(file_names,(basedir+'ch_cam'+str(sst)+'ri0_4096x64x64'\n",
    "                                           +'_3km_12s_cam'+str(sst)+'ri0_64_'+suffix+'.nc'))\n",
    "        \n",
    "    return file_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data into Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of files\n",
    "files = file_concatenator(300,10,1800,721800,1800)\n",
    "\n",
    "#choose the 200th file of 400 to consider initially, while developing the code\n",
    "file_init = xr.open_dataset(files[199])\n",
    "\n",
    "#call temperature (absolute)\n",
    "temp = file_init.TABS\n",
    "\n",
    "#precipitating water\n",
    "precip = file_init.QP\n",
    "\n",
    "#non precipitating water\n",
    "qn = file_init.QN\n",
    "\n",
    "qv = file_init.QV\n",
    "\n",
    "ppp = file_init.p\n",
    "\n",
    "#qn245 = qn[0,24]\n",
    "#plt.figure(figsize=(10,5))\n",
    "#plt.hist(qn245.data.flatten(),bins=np.logspace(-10,1,50))\n",
    "#plt.gca().set_xscale('log')\n",
    "#plt.show()\n",
    "# It seems no files have negative values in qn, so i'm not sure if this is meant to be used later on for q_245\n",
    "\n",
    "#print(file_init)\n",
    "\n",
    "def variables_get(file_obj,var_names):\n",
    "    \n",
    "    output = [[]]*len(var_names)\n",
    "    \n",
    "    for i,var_name in enumerate(var_names):\n",
    "        \n",
    "        \n",
    "        output[i] = file_obj[var_name]\n",
    "        \n",
    "    return output\n",
    "\n",
    "tt,qqpp,qqnn = variables_get(file_init,['TABS','QP','QN'])\n",
    "\n",
    "#print(tt)\n",
    "\n",
    "#print(qqnn)\n",
    "\n",
    "ppp = ppp[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying first filter for rainy grid cells\n",
    "\n",
    "## NOT BEING APPLIED RIGHT NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the surface (z=0) level values\n",
    "psurf = precip[0,1]\n",
    "\n",
    "#setting up a binary array\n",
    "mcs_0 = np.ones((64,4096))\n",
    "\n",
    "#where surface precipitation is 0, binary array = 0\n",
    "#mcs = np.where(psurf==0,0,mcs)\n",
    "\n",
    "#calling the mcs boolean mesh to view at this stage\n",
    "#colour mesh of mcs\n",
    "#fig,ax = plt.subplots(figsize = (64*4,4))\n",
    "\n",
    "#ax.pcolormesh(mcs,cmap='Greys')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second filter for aloft condensate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def search_z(arr_4d_0,arr_4d_1,time_index,less_than_val):\n",
    "    #similar function to search_z, iterates over all z, in reverse, filling in \n",
    "    #an otherwise nan filled array with values from arr_4d_1, of indices corresponding\n",
    "    #to values in arr_4d_0 satisfying a critical value relation\n",
    "    \n",
    "    #both input arrays have structure time,z,y,x (hence 4d)\n",
    "    #time index generalised but only 0 here\n",
    "    #critical_val is a value elements must be greater or equal to in the principal array  \n",
    "\n",
    "    #array of nans constructed, 2d array of y and x\n",
    "    arr_2d = nan_array(np.shape(arr_4d_0[time_index,0]))\n",
    "    \n",
    "    #find length of z array\n",
    "    z_len = len(arr_4d_0[time_index,:,0,0])\n",
    "\n",
    "    #iterate from last z values first\n",
    "    for z_i in np.arange(0,z_len):\n",
    "    #iterate over all z levels\n",
    "    \n",
    "        #print(z_i) #counter\n",
    "\n",
    "        #check that all elements haven't been filled\n",
    "        if len(np.where(arr_4d_0[time_index,z_i,:,:] < less_than_val)[0]) > 0:\n",
    "    \n",
    "            empty_indices = np.where(np.isnan(arr_2d)==True)\n",
    "            \n",
    "            #print(len(empty_indices[0])) # filling check\n",
    "            \n",
    "    \n",
    "            #check that all elements haven't been filled\n",
    "            if len(empty_indices[0]) > 0:\n",
    "            \n",
    "                #print('##########')  #signal start of filling\n",
    "            \n",
    "                #replace only the empty array sites using corresponding\n",
    "                #subset of arr_4d_0 and 1\n",
    "                arr_2d[empty_indices] = np.where(arr_4d_0.data[time_index,z_i][empty_indices] < less_than_val,\n",
    "                                                 arr_4d_1.data[time_index,z_i][empty_indices],\n",
    "                                                 arr_2d[empty_indices])\n",
    "                \n",
    "            else:\n",
    "            \n",
    "                break\n",
    "    \n",
    "    return arr_2d\n",
    "\n",
    "\n",
    "\n",
    "#call z search function for temp and qn\n",
    "qn_245_2 = search_z(temp,qn,0,245)\n",
    "\n",
    "#print(qn_245_2)\n",
    "\n",
    "#set mcs values < 0 to 0 in boolean array\n",
    "mcs_0[np.where(qn_245_2<10**(-4))] = 0\n",
    "\n",
    "\"\"\"\n",
    "#zero values less than 0...?\n",
    "#print(len(np.where(qn_245_2<10**(-4))[0]))\n",
    "\n",
    "#colormesh of qn_245_2\n",
    "fig,ax = plt.subplots(figsize = (64*4,4))\n",
    "\n",
    "#showing the qn_245 array\n",
    "mesh_qn = ax.pcolormesh(qn_245_2,cmap='Greys')\n",
    "\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third filter for contiguous coverage of condensate with radius of 90km or more\n",
    "\n",
    "First showing the MCS boolean array after treating for condensates, without clusters located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate figure with correct proportions\n",
    "fig,ax = plt.subplots(figsize = (64*4,4))\n",
    "\n",
    "ax.pcolormesh(mcs_0,cmap='Greys')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function is created to cluster connected components and then restrict\n",
    "these based on their size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def clusters(arr,size):\n",
    "    #function using the scipy ndimage package to find connected\n",
    "    #components in the inputted boolean array and then also outputs\n",
    "    #clusters only above a certain pixel size threshold\n",
    "    \n",
    "    #arr is boolean array, size is the threshold size which clusters\n",
    "    #need to be above\n",
    "    \n",
    "    #outputs the original object labels and the number of clusters\n",
    "    #labels_0 and nb_0 respectively.Then the corresponding reduced \n",
    "    #array and cluster number, labels_1 and nb_1 are outputted\n",
    "    \n",
    "    #convert Boolean array into integers\n",
    "    arr = arr.astype('int32')\n",
    "    \n",
    "    #call the connected components and number of clusters using ndimage\n",
    "    #uses 4 point connectivity\n",
    "    labels_0,nb_0 = ndimage.label(arr)\n",
    "    \n",
    "    #create new duplicate local variable labels_1, the edited array\n",
    "    labels_1 = copy.deepcopy(labels_0)\n",
    "    \n",
    "    #find the sizes of each object, in terms of pixels \n",
    "    pixelsizes = ndimage.sum(arr,labels_1,range(nb_0+1))\n",
    "\n",
    "    #create a boolean array for whether the objects \n",
    "    #are larger than a given threshold\n",
    "    mask_size = pixelsizes < size\n",
    "    \n",
    "    nb_1 = len(np.where(mask_size==False)[0])\n",
    "\n",
    "    #indices of pixels to be removed are produced\n",
    "    remove_pixel = mask_size[labels_1]\n",
    "    \n",
    "    #indices and thus objects are removed based on object size\n",
    "    labels_1[remove_pixel] = 0\n",
    "    \n",
    "    return [labels_0,nb_0],[labels_1,nb_1]\n",
    "\n",
    "\n",
    "clusters_0,clusters_1 = clusters(mcs_0,310)\n",
    "\n",
    "\n",
    "fig,(ax1,ax2,ax3) = plt.subplots(3,1,figsize=(64*4,14))\n",
    "\n",
    "ax1.imshow(mcs_0,cmap='Greys')\n",
    "\n",
    "ax2.imshow(clusters_0[0])\n",
    "\n",
    "ax3.imshow(clusters_1[0])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markdown to save time\n",
    "### Finding number of storms\n",
    "The previous functions will be ran in a main() type function, iterating over the file names, to find the total number of storm objects at this stage of filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def storm_collector(file_list):\n",
    "    \n",
    "    storm_number = np.array([],dtype='int')\n",
    "\n",
    "    for i,file_name in enumerate(file_list):\n",
    "        \n",
    "        file = xr.open_dataset(file_name)\n",
    "    \n",
    "        #call temperature (absolute)\n",
    "        temp = file.TABS\n",
    "\n",
    "        #precipitating water\n",
    "        precip = file.QP\n",
    "\n",
    "        #non precipitating water\n",
    "        qn = file.QN\n",
    "    \n",
    "        #setting up a binary array\n",
    "        binary = np.ones((64,4096))\n",
    "    \n",
    "        #PRECIPITATION FILTER NOT APPLIED FIRST\n",
    "    \n",
    "        #calling the surface (z=0) level values\n",
    "        #psurf = precip[0,0]\n",
    "    \n",
    "        #where surface precipitation is 0, binary array = 0\n",
    "        #mcs = np.where(psurf==0,0,mcs)\n",
    "        \n",
    "        #qn values corresponding to lowest vertical position where temp < 245\n",
    "        qn_245 = search_z(temp,qn,0,245)\n",
    "        \n",
    "        #set mcs values < 0 to 0 in boolean array\n",
    "        binary[np.where(qn_245<10**(-4))] = 0\n",
    "        \n",
    "        objects,storms = clusters(binary,310)\n",
    "        \n",
    "        print(i,storms[1])\n",
    "        \n",
    "        storm_number = np.append(storm_number,storms[1])\n",
    "        \n",
    "        \n",
    "    \n",
    "    storm_tot = np.cumsum(storm_number)\n",
    "    \n",
    "    storm_mean = np.mean(storm_number)\n",
    "    \n",
    "    storm_std = np.std(storm_number)\n",
    "    \n",
    "    return storm_tot,storm_mean,storm_std\n",
    "\n",
    "total,mean,std = storm_collector(files) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(total,mean,std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ON HOLD\n",
    "\n",
    "#### Calculating storm depth\n",
    "\n",
    "STORM DEPTH ON HOLD: \n",
    "-mesh still only a block of blue color\n",
    "- still lots of low values\n",
    "\n",
    "- also depth is changed ( i believe erroneously) with:\n",
    "\n",
    "        depth = np.where(mcs==1,0,depth)\n",
    "        \n",
    "  surely this removes points where the binary array is true?However when you change it to mcs == 0 , or just so that \n",
    "  points over mcs false are then 0, the stats decrease massively giving a mean of 4, with all first 3 quartiles at 0..?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "def search_z_2(arr_4d_0,arr_4d_1,time_index,critical_val):\n",
    "    #similar function to search_z, iterates over all z, in reverse, filling in \n",
    "    #an otherwise nan filled array with values from arr_4d_1, of indices corresponding\n",
    "    #to values in arr_4d_0 satisfying a critical value relation\n",
    "    \n",
    "    #both input arrays have structure time,z,y,x (hence 4d)\n",
    "    #time index generalised but only 0 here\n",
    "    #critical_val is a value elements must be greater or equal to in the principal array  \n",
    "\n",
    "    #array of nans constructed, 2d array of y and x\n",
    "    arr_2d = nan_array(np.shape(arr_4d_0[time_index,0]))\n",
    "\n",
    "    #iterate from last z values first\n",
    "    for z_i in reversed(np.arange(0,len(arr_4d_0[time_index,:,0,0]))):\n",
    "        #iterate over all z levels\n",
    "    \n",
    "        #print(z_i)\n",
    "    \n",
    "        if len(np.where(arr_4d_0[time_index,z_i,:,:] >= critical_val)[0]) > 0:\n",
    "            \n",
    "            #print('###########################')\n",
    "            \n",
    "            #finding indices to replace\n",
    "            empty_indices = np.where(np.isnan(arr_2d)==True)\n",
    "            \n",
    "            #print(len(empty_indices[0]))\n",
    "    \n",
    "            #check that all elements haven't been filled\n",
    "            if len(empty_indices[0]) > 0:\n",
    "                \n",
    "                #replace only the empty array sites using corresponding\n",
    "                #subset of arr_4d_0 and 1\n",
    "                arr_2d[empty_indices] = np.where(arr_4d_0.data[time_index,z_i][empty_indices] >= critical_val,\n",
    "                                                 arr_4d_1.data[time_index,z_i][empty_indices],\n",
    "                                                 arr_2d[empty_indices]) \n",
    "    \n",
    "    return arr_2d\n",
    "\n",
    "ctt = search_z_2(qn,temp,0,10**(-4))\n",
    "\n",
    "#print(np.shape(ctt))\n",
    "\n",
    "ts = temp[0,0,:,:]\n",
    "\n",
    "depth = ts.data - ctt\n",
    "\n",
    "print(np.shape(depth[~np.isnan(depth)]))\n",
    "\n",
    "#depth = np.where(mcs==1,0,depth)\n",
    "\n",
    "#print(np.shape(depth))\n",
    "\n",
    "#colormesh of qn_245_2\n",
    "fig,ax = plt.subplots(figsize = (64*4,4))\n",
    "\n",
    "#showing the qn_245 array\n",
    "ax.pcolormesh(depth[~np.isnan(depth)],cmap='Greens')\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(np.nanmean(depth),\n",
    "      np.nanpercentile(depth,0),\n",
    "      np.nanpercentile(depth,25),\n",
    "      np.nanmedian(depth),\n",
    "      np.nanpercentile(depth,75),\n",
    "      np.nanpercentile(depth,100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving data for specific storm sites\n",
    "Initial precipitation filter has been removed, using aloft condensate to set the principal values of the binary array, this produced a more reasonable number of storms.\n",
    "\n",
    "Now data values need to be extracted and produced for each storm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering the file before the input\n",
    "#file_two = xr.open_dataset(files[199])\n",
    "\n",
    "qv_2 = file_init.QV\n",
    "#qv_2 = file_two.QV\n",
    "\n",
    "#non precipitating water\n",
    "qn_2 = file_init.QN\n",
    "#qn_2 = file_two.QN\n",
    "\n",
    "#pressure\n",
    "press_2 = file_init.p\n",
    "#press_2 = file_two.p\n",
    "\n",
    "z_2 = file_init.z\n",
    "#z_2 = file_two.z\n",
    "\n",
    "#call temperature (absolute)\n",
    "temp_2 = file_init.TABS\n",
    "#temp_2 = file_two.TABS\n",
    "\n",
    "#calculate environmental variables, averaging over x and y for each z level\n",
    "\n",
    "tenv = np.mean(np.mean(temp_2[0],1),1)\n",
    "\n",
    "qvenv = np.mean(np.mean(qv_2[0],1),1)\n",
    "\n",
    "qnenv = np.mean(np.mean(qn_2[0],1),1)\n",
    "\n",
    "#calculate buoyancy profile using environmental variables\n",
    "\n",
    "buoy = g*(temp_2 - tenv)/tenv + (MWw/MWa)*(qvenv - qv_2) - (qnenv - qn_2)\n",
    "\n",
    "buoy = buoy[0]\n",
    "\n",
    "print(buoy[:,0,0])\n",
    "\n",
    "buoy = np.where(buoy<0,0,buoy)\n",
    "\n",
    "print(buoy[:,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#buoyancy function not currently working\n",
    "\n",
    "def buoy_function(t_arr,qv_arr,qn_arr):\n",
    "    \n",
    "    t_env = np.mean(np.mean(t_arr[0],1),1)\n",
    "    \n",
    "    #print(t_env)\n",
    "\n",
    "    qv_env = np.mean(np.mean(qv_arr[0],1),1)\n",
    "    \n",
    "    #print(qv_env)\n",
    "\n",
    "    qn_env = np.mean(np.mean(qn_arr[0],1),1)\n",
    "    \n",
    "    #print(qn_env)\n",
    "    \n",
    "    buoyancy = g*(t_arr - t_env)/t_env + (MWw/MWa)*(qv_env - qv_arr) - (qn_env - qn_arr)\n",
    "    \n",
    "    buoyancy = np.where((buoyancy<0,0,buoyancy))\n",
    "    \n",
    "    return buoyancy\n",
    "  \n",
    "    \n",
    "buoyancy = buoy_function(temp_2,qv_2,qn_2)\n",
    "\n",
    "print(buoyancy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def field_plots(clusters,raw,var_name_unit):\n",
    "    \n",
    "    fig,axes = plt.subplots(3,1,figsize=(64*4,6))\n",
    "\n",
    "    labels = ['Raw {} Field'.format(var_name_unit[0]),'Storm Filtered {} Field'.format(var_name_unit[0])]\n",
    "    \n",
    "    filtered = np.where(clusters==0,0,raw)\n",
    "     \n",
    "    \n",
    "    im1 = axes[0].imshow(clusters,cmap='Greys',vmin=5,vmax=10)\n",
    "                 \n",
    "    axes[0].set_title('Storm Clusters')\n",
    "            \n",
    "\n",
    "                 \n",
    "    im2 = axes[1].imshow(raw,vmin=np.min(raw),vmax=np.max(raw))\n",
    "            \n",
    "    axes[1].set_title(labels[0])\n",
    "    \n",
    "    axes[1].set_ylabel('Y Coordinates [$km$]',size=18)\n",
    "    \n",
    "    \n",
    "    im3 = axes[2].imshow(filtered,vmin=np.min(raw),vmax=np.max(raw))\n",
    "    \n",
    "    axes[2].set_title(labels[1])\n",
    "    \n",
    "    axes[2].set_xlabel('X Coordinates [$km$]',size=18)\n",
    "\n",
    "    \n",
    "    #space between plots adjusted\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    \n",
    "    plt.colorbar(im2, ax=axes.ravel().tolist(),pad=0.001,label='{0} [${1}$]'.format(var_name_unit[0],var_name_unit[1]))\n",
    "    \n",
    "    plt.suptitle('Field Outputs: Clusters vs Raw {} Field'.format(var_name_unit[0]),x=0.6375,y=1.01,size=18)\n",
    " \n",
    "    plt.show()\n",
    "            \n",
    "    return filtered\n",
    "        \n",
    "buoy_collocated = field_plots(clusters_1[0],buoy[63],['Buoyancy','N'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(buoy[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sub_arr_flat(arr):\n",
    "    \n",
    "    #print(np.shape(arr))\n",
    "    \n",
    "    xy_len = len(arr[0].flatten())\n",
    "    #print(xy_len)\n",
    "    \n",
    "    z_len = len(arr[:,0,0])\n",
    "    #print(z_len)\n",
    "    \n",
    "    arr_flat = np.empty((z_len,xy_len))\n",
    "    \n",
    "    for i in np.arange(z_len):\n",
    "        \n",
    "        arr_flat[i] = arr[i].flatten()\n",
    "    \n",
    "    return arr_flat,xy_len\n",
    "      \n",
    "def integrate_z(arr,z_vals):\n",
    "    \n",
    "    arr_flat,length = sub_arr_flat(arr)\n",
    "    \n",
    "    #print(np.shape(arr_flat))\n",
    "    \n",
    "    arr_out = np.empty(length)\n",
    "    \n",
    "    for i in np.arange(0,length):\n",
    "        \n",
    "        arr_out[i] = np.trapz(x=z_vals,y=arr_flat[:,i])\n",
    "        \n",
    "    return np.reshape(arr_out,(np.shape(arr[0])))\n",
    "        \n",
    "cape = integrate_z(buoy,z_2.data)\n",
    "\n",
    "cape_collocated = field_plots(clusters_1[0],cape,['CAPE','J Kg^{-1}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ppp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(qv[0][:,0,0])\n",
    "cwvc_integral = integrate_z(qv[0].data,np.array(ppp.data)*100)\n",
    "\n",
    "print(cwvc_integral)\n",
    "\n",
    "#print(np.shape(cwvc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(np.where(cwvc_integral<0)[0]))\n",
    "\n",
    "cwvc = cwvc_integral/g\n",
    "\n",
    "cwvc_collocated = field_plots(clusters_1[0],cwvc,['CWVC','-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def find_less(array,value,number):\n",
    "    #function to find 'number' of initial values in 'array' greater\n",
    "    #than some 'value'\n",
    "    \n",
    "    #array here has descending values, designed for pressure array going\n",
    "    #from 1000 to 14\n",
    "    \n",
    "    #indices of values in array greater than the user value\n",
    "    k = np.where(array>value)\n",
    "    \n",
    "    #number of values greater than the value to be chosen\n",
    "    #i.e. 1 gives the value closest, 2 gives the two closes\n",
    "    values = (array[k])[-number:]\n",
    "    \n",
    "    indices = k[0][-number:]\n",
    "        \n",
    "    return indices,values.data\n",
    "\n",
    "def satVapP_liq(t_in):\n",
    "    R = 8.314             # J mol-1 K-1\n",
    "    MWw = 18.015/1000     # kg mol-1\n",
    "    rhoa = 1.395\n",
    "    a1 = 54.842763\n",
    "    a2 = -6763.22\n",
    "    a3 = -4.21\n",
    "    a4 = 0.000367\n",
    "    a5 = 0.0415\n",
    "    a6 = 218.8\n",
    "    a7 = 53.878\n",
    "    a8 = -1331.22\n",
    "    a9 = -9.44523\n",
    "    a10 = 0.014025\n",
    "    factor = a7 + a8/t_in + a9*np.log(t_in) + a10*t_in\n",
    "    psatL = a1 + a2/t_in + a3*np.log(t_in) + a4*t_in + np.arctan(a5*(t_in - a6))*factor\n",
    "    psatL = np.exp(psatL)\n",
    "    return psatL\n",
    "\n",
    "def sd(qv_value,p_value,t_value):\n",
    "    #function to calculate saturation deficit given water content,pressure and temperature\n",
    "    \n",
    "    return (eps*satVapP_liq(t_value))/(p_value*100 - satVapP_liq(t_value))*1000 - qv_value\n",
    "\n",
    "def sd_plev(p_arr,p_val,qv_arr,t_arr):\n",
    "    \n",
    "    arrs_vals = np.empty((2,64,4096))\n",
    "    \n",
    "    idx,p = find_less(p_arr,p_val,1)\n",
    "\n",
    "    for i,arr in enumerate([qv_arr,t_arr]):\n",
    "        \n",
    "        arr_val = arr[0,idx].data\n",
    "        \n",
    "        arrs_vals[i] = arr_val\n",
    "        \n",
    "    return sd(arrs_vals[0],p,arrs_vals[1])\n",
    "\n",
    "def sd_mean(p_arr,plevs,qv_arr,temp_arr):\n",
    "    \n",
    "    sd_arrs = np.empty((3,64,4096))\n",
    "    \n",
    "    for i in range(len(sd_arrs)):\n",
    "        \n",
    "        sd_arrs[i] = sd_plev(p_arr,plevs[i],qv_arr,temp_arr)\n",
    "\n",
    "    \n",
    "    return np.nanmean(sd_arrs,0)\n",
    "\n",
    "sd_avg = sd_mean(press_2,[550,700,850],qv_2,temp_2)\n",
    "\n",
    "sd_collocated = field_plots(clusters_1[0],sd_avg,['Saturation Deficit','-'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numbin = 100\n",
    "\n",
    "#temps = np.linspace(np.nanmin(sd_avg),np.nanmax(sd_avg),numbin)\n",
    "\n",
    "\n",
    "psurf_collocated = field_plots(clusters_1[0],psurf,['Precipitation','mm h^{-1}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def data_xr(file_names):\n",
    "\n",
    "    for k,file_name in enumerate(file_names):\n",
    "        \n",
    "        file = xr.open_dataset(file_name)\n",
    "        \n",
    "        ################################\n",
    "        #calling file variables\n",
    "        ################################\n",
    "        \n",
    "        #call temperature (absolute)\n",
    "        temp = file.TABS\n",
    "\n",
    "        #precipitating water\n",
    "        precip = file.QP\n",
    "        \n",
    "        #calling the surface (z=0) level values\n",
    "        psurf = precip[0,0]\n",
    "\n",
    "        #non precipitating water\n",
    "        qn = file.QN\n",
    "        \n",
    "        #precipitating water content\n",
    "        qv = file.QV\n",
    "\n",
    "        #pressure\n",
    "        press = file.p\n",
    "        \n",
    "        #z vals\n",
    "        z = file.z\n",
    "\n",
    "        #call temperature (absolute)\n",
    "        temp = file.TABS\n",
    "        \n",
    "        ################################\n",
    "        #filtering \n",
    "        ################################\n",
    "        \n",
    "        #setting up a binary array\n",
    "        mcs = np.ones((64,4096))\n",
    "        \n",
    "        #call z search function for temp and qn\n",
    "        qn_245 = search_z(temp,qn,0,245)\n",
    "\n",
    "        #set mcs values < 0 to 0 in boolean array\n",
    "        mcs[np.where(qn_245<10**(-4))] = 0\n",
    "        \n",
    "        ######################################################################\n",
    "        #clustering and collocating\n",
    "        ######################################################################\n",
    "       \n",
    "        ##########\n",
    "        #Clustering\n",
    "        ##########\n",
    "        \n",
    "        clusters_,clusters_storms = clusters(mcs,310)\n",
    "        \n",
    "        ###########\n",
    "        #Buoyancy\n",
    "        ###########\n",
    "        \"\"\"\n",
    "        Buoyancy calculation should be in a function but cannot seem to get one to work, hence\n",
    "        the many lines\n",
    "        \"\"\"\n",
    "        \n",
    "        #calculate environmental variables, averaging over x and y for each z level\n",
    "        tenv = np.mean(np.mean(temp[0],1),1)\n",
    "\n",
    "        qvenv = np.mean(np.mean(qv[0],1),1)\n",
    "\n",
    "        qnenv = np.mean(np.mean(qn[0],1),1)\n",
    "\n",
    "        #calculate buoyancy profile using environmental variables\n",
    "        buoy = g*(temp - tenv)/tenv + (MWw/MWa)*(qvenv - qv) - (qnenv - qn)\n",
    "\n",
    "        ### buoy = buoy[0]\n",
    "    \n",
    "        #filtering and collocating buoyancy\n",
    "        buoy = np.where(buoy[0]<0,0,buoy[0])\n",
    "\n",
    "        buoy_col = np.where(clusters_storms[0]==0,0,buoy)\n",
    "        \n",
    "        #############\n",
    "        #cape\n",
    "        #############\n",
    "        \n",
    "        cape = integrate_z(buoy,z.data)\n",
    "        \n",
    "        cape_col = np.where(clusters_storms[0]==0,0,cape)\n",
    "        \n",
    "        ##############\n",
    "        #saturation deficit\n",
    "        ##############\n",
    "        \n",
    "        sd_avg = sd_mean(press,[550,700,850],qv,temp)\n",
    "        \n",
    "        sd_avg_col = np.where(clusters_storms[0]==0,0,sd_avg)\n",
    "        \n",
    "        ################\n",
    "        #psurf\n",
    "        ################\n",
    "        \n",
    "        psurf_col = np.where(clusters_storms[0]==0,0,psurf)\n",
    "        \n",
    "        \n",
    "        ###################################################\n",
    "        #arranging data by \n",
    "        \n",
    "        #print(clusters_storms[0])\n",
    "\n",
    "        vals_unique = np.unique(clusters_storms[0])\n",
    "        \n",
    "        #print(vals_unique)\n",
    "\n",
    "        #0 not considered\n",
    "        vals_unique = vals_unique[1:]\n",
    "        \n",
    "        #print(vals_unique)\n",
    "\n",
    "        output_arr = np.empty((len(vals_unique),4))\n",
    "        \n",
    "        #print(output_arr)\n",
    "\n",
    "        for i,val in enumerate(vals_unique):\n",
    "            \n",
    "            #print(np.shape(buoy_col),np.shape(cape_col),np.shape(sd_avg_col),np.shape(psurf_col))\n",
    "\n",
    "            #find indices associated with the numerical tag,corresponding to 1 storm\n",
    "            indices = np.where(clusters_storms[0] == val)\n",
    "\n",
    "            #extract the variable values from these desired elements            \n",
    "            output_arr[i] = [np.mean(buoy_col[0][indices]),np.mean(cape_col[indices]),\n",
    "                             np.mean(sd_avg_col[indices]),np.max(psurf_col[indices])]\n",
    "            \n",
    "            #print(np.shape(buoy_col),np.shape(cape_col),np.shape(sd_avg_col),np.shape(psurf_col))\n",
    "        \n",
    "        \n",
    "        if k == 0:\n",
    "            \n",
    "            super_output = output_arr\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            super_output = np.vstack((super_output,output_arr))\n",
    "\n",
    "    #ds = xr.Dataset(\n",
    "     #   {\"buoyancy\": super_output[:,0]},\n",
    "      #  {\"cape\": super_output[:,1]},\n",
    "       # {\"sd\": super_output[:,2]},\n",
    "        #{\"pmax\": super_output[:,3]})\n",
    "\n",
    "    #ds.to_netcdf(\"collocated_storms_all.nc\")\n",
    "    \n",
    "    return super_output\n",
    "\n",
    "big_data = data_xr(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('buoy_cape_sd_pmax_300',big_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxk_arg(matrix,k):\n",
    "    #returns indices of max k elements in a matrix\n",
    "    \n",
    "    matrix_new = matrix.flatten()\n",
    "    \n",
    "    matrix_arg = np.argsort(matrix_new)\n",
    "    \n",
    "    return matrix_arg[-k:]\n",
    "\n",
    "\n",
    "def bin_stat_function(n_bins,lower,upper,x_variable,y_variable,threshold=0,pc1=95,pc2=99.99,n_max=5):\n",
    "    #Creates a range of bin values within which the data should lie, collects indices of x-variables\n",
    "    #which fall in those bins and calls the corresponding y-variable values, calculates means\n",
    "    #and percentiles. \n",
    "    \n",
    "    #n_bins = number of bins, lower and upper = bounds of bins, threshold is a required number\n",
    "    #of values to have if statistics are to be calculated, pc1,2 are percentiles, assumed 95 and 99\n",
    "    \n",
    "    cc = np.linspace(lower,upper,n_bins)\n",
    "    \n",
    "    #nan filled arrays created\n",
    "    \n",
    "    x_bins = nan_array((n_bins,1))\n",
    "    y_bins = nan_array((n_bins,1))\n",
    "    \n",
    "    y_bins_pc1 = nan_array((n_bins,1))\n",
    "    y_bins_pc2 = nan_array((n_bins,1))\n",
    "    \n",
    "    x_max = nan_array((n_bins,n_max))\n",
    "    y_max = nan_array((n_bins,n_max))\n",
    "    \n",
    "    x_bins_error = nan_array((n_bins,1))\n",
    "    y_bins_error = nan_array((n_bins,1))\n",
    "    \n",
    "    \n",
    "    #reduced bin values for the loop below\n",
    "    cc_red = cc[:-1]\n",
    "    \n",
    "    for i,value in enumerate(cc_red):\n",
    "        \n",
    "        #indices of values within bins\n",
    "        j = np.where((x_variable >= value) & (x_variable < cc[i+1]))\n",
    "\n",
    "        x_vals = x_variable[j]\n",
    "        y_vals = y_variable[j]\n",
    "\n",
    "        #threshold inspected    \n",
    "        if len(j[0]) > threshold:\n",
    "            \n",
    "            #mean of x values within bin\n",
    "            x_bins[i] = np.nanmean(x_vals)\n",
    "            x_bins_error[i] = np.nanstd(x_vals)   \n",
    "            \n",
    "            #mean and percentiles of associated y variable\n",
    "            y_bins[i] = np.nanmean(y_vals)\n",
    "            y_bins_error[i] = np.nanstd(y_vals)\n",
    "    \n",
    "            y_bins_pc1[i] = np.nanpercentile(y_vals,pc1)\n",
    "            y_bins_pc2[i] = np.nanpercentile(y_vals,pc2)\n",
    "\n",
    "        if len(j[0]) >= n_max:\n",
    "        \n",
    "            j = maxk_arg(y_vals,n_max)\n",
    "\n",
    "            \n",
    "            x_max[i] = x_vals[j]\n",
    "            \n",
    "            y_max[i] = y_vals[j]\n",
    "    \n",
    "\n",
    "    return x_bins,y_bins,y_bins_pc1,y_bins_pc2,x_bins_error,y_bins_error,x_max,y_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(big_data[:,2],big_data[:,1])\n",
    "plt.show()\n",
    "\n",
    "sd_bins,cape_mean,cape_95,cape_99,sd_err_x,cape_err_y,sd_max_x,cape_max_y = bin_stat_function(100,0.5,5,big_data[:,2],\n",
    "                                                                                      big_data[:,1],threshold=0,\n",
    "                                                                                      pc1=95,pc2=99.99,n_max=2)\n",
    "\n",
    "#plt.errorbar(sd_bins,cape_mean,xerr=sd_err_x,yerr=cape_err_y,color='k',ls='none')\n",
    "#plt.scatter(sd_bins,cape_95,color='orange')\n",
    "plt.scatter(sd_bins,cape_99,color='red')\n",
    "#plt.scatter(sd_max,cape_max,color='purple')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(big_data[:,1],big_data[:,3])\n",
    "plt.show()\n",
    "\n",
    "cape_bins,pmax_mean,pmax_95,pmax_99,cape_err_x,pmax_err_y,cape_max_x,pmax_max_y = bin_stat_function(85,0,3500,big_data[:,1],\n",
    "                                                                                      big_data[:,3],threshold=0,\n",
    "                                                                                      pc1=95,pc2=99.99,n_max=2)\n",
    "\n",
    "plt.errorbar(cape_bins,pmax_mean,xerr=cape_err_x,yerr=pmax_err_y,color='k',ls='none')\n",
    "#plt.scatter(cape_bins,pmax_95,color='orange')\n",
    "#plt.scatter(cape_bins,pmax_99,color='red')\n",
    "#plt.scatter(cape_max_x,pmax_max_y,color='purple')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(big_data[:,2],big_data[:,3])\n",
    "plt.show()\n",
    "\n",
    "sd_p_bins,pmax_sd_mean,pmax_sd_95,pmax_sd_99,sd_p_err_x,pmax_sd_err_y,sd_p_max_x,pmax_sd_max_y = bin_stat_function(85,0,2.5,big_data[:,2],\n",
    "                                                                                      big_data[:,3],threshold=0,\n",
    "                                                                                      pc1=95,pc2=99.99,n_max=2)\n",
    "\n",
    "plt.errorbar(sd_p_bins,pmax_sd_mean,xerr=sd_p_err_x,yerr=pmax_sd_err_y,color='k',ls='none')\n",
    "#plt.scatter(sd_p_bins,pmax_sd_95,color='orange')\n",
    "#plt.scatter(sd_p_bins,pmax_sd_99,color='red')\n",
    "plt.scatter(sd_p_max_x,pmax_sd_max_y,color='purple')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('buoy_cape_sd_pmax_300.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "\n",
    "- IMPORTANT nan argsort in era5 notebook, nan argsort doesnt find the values because there are nan values being chosen as max values\n",
    "\n",
    "- change sd scaling by 1000, cut off at 0 .... buoyancy model only takes positive sd (in ERA 5 file)\n",
    "\n",
    "- when other sst files are uploaded, save as npy files\n",
    "\n",
    "- load new npy files into a master analysis notebook,then perform the bin analysis using the combined sst files\n",
    "\n",
    "- make sure to retain individual sst files to see if trends are similar over different ssts\n",
    "\n",
    "- w,p loading \n",
    "\n",
    "- create cwvc variable function in draft notebook (THIS) , with a field as usual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmax=10\n",
    "\n",
    "pairs = []\n",
    "\n",
    "for k in np.arange(2,kmax):\n",
    "    \n",
    "        #print((k-1)*np.ones(kmax-(k-1)))\n",
    "        #print(pairs)\n",
    "        \n",
    "        if k == 2:\n",
    "            \n",
    "            pairs = [np.arange(k,kmax),(k-1)*np.ones(kmax-(k-1))]\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            pairs = np.vstack((pairs,[np.arange(k,kmax),(k-1)*np.ones(kmax-(k-1))]))\n",
    "            \n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z = np.random.rand(10,5)\n",
    "print(z)\n",
    "\n",
    "x = np.arange(0,6,1)\n",
    "\n",
    "y = np.arange(0,11,1)\n",
    "\n",
    "plt.pcolormesh(x,y,z)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(len(variable.x.data),len(variable.y.data),np.shape(variable.QV.data))\n",
    "\n",
    "#fig,ax = plt.subplots()\n",
    "\n",
    "#ax.pcolormesh(variable.x.data,variable.y.data,variable.QV.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clusterXYpoints(input_file,max_dist=10,min_cluster_size=1,merge_flag=True):\n",
    "    \n",
    "    xy = xr.open_dataset(file)\n",
    "    \n",
    "    d = np.array(pdist(xy,'euclidean'))\n",
    "    \n",
    "    #pairs = np.empty()\n",
    "    \n",
    "    kmax = np.size(xy,1)\n",
    "    \n",
    "    for k in np.arange(2,kmax):\n",
    "        \n",
    "        if k == 2:\n",
    "            \n",
    "            pairs = [np.arange(k,kmax),(k-1)*np.ones(kmax-(k-1))]\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            pairs = np.vstack(pairs,[np.arange(k,kmax),\n",
    "                                     (k-1)*np.ones(kmax-(k-1))])\n",
    "            \n",
    "    i_sorted = np.argsort(d)\n",
    "    \n",
    "    d_sorted = d[i_sorted]\n",
    "    pairs_sorted = pairs[i_sorted,:]\n",
    "    \n",
    "    clusters = {}\n",
    "    \n",
    "    cluster_points = []\n",
    "    \n",
    "    inc = 0\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5 != 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.zeros(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_array(shape):\n",
    "    \n",
    "    an_array = np.empty(shape)\n",
    "\n",
    "    an_array[:] = np.NaN\n",
    "    \n",
    "    return an_array\n",
    "\n",
    "x = nan_array((10))\n",
    "\n",
    "print(np.where(np.isnan(x)==True,5,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "qn_245 = np.zeros((64,4096))\n",
    "\n",
    "print(np.size(qn_245))\n",
    "\n",
    "x = np.where(qn_245 ==0)\n",
    "\n",
    "print(len(x[1]))\n",
    "\n",
    "#print(np.where(temp[0,5,:,:] > 0))\n",
    "\n",
    "\n",
    "for z_i in np.arange(0,64):\n",
    "    \n",
    "    tb_245 = np.where(temp[0,z_i,:,:].data < 245)\n",
    "    \n",
    "    print(len(tb_245[0]))\n",
    "    \n",
    "    print('################')\n",
    "    \n",
    "    #print(np.shape(tb_245))\n",
    "    \n",
    "    qn_245 = qn[0,z_i,tb_245[0],tb_245[1]]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x = np.random.randint(100,size=(10,10,10))\n",
    "\n",
    "print(np.shape(x))\n",
    "#print((x))\n",
    "\n",
    "for z_i in np.arange(0,10):\n",
    "    \n",
    "    z_val = np.where(x[z_i,:,:] > 50)\n",
    "    \n",
    "    print(z_val[0],z_val[1])\n",
    "    print('###############################')\n",
    "\n",
    "\n",
    "#z = np.where(x<50,-1,x)\n",
    "\n",
    "#print('###############################')\n",
    "#print(z)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "for x in np.arange(0,4096):\n",
    "    \n",
    "    print(x)\n",
    "    \n",
    "    for y in np.arange(0,64):\n",
    "        \n",
    "        print(y)\n",
    "        \n",
    "        tb_245 = np.where(temp[0,:,y,x]<245)[0][0]\n",
    "        \n",
    "        qn_245[y,x] = qn[0,tb_245,y,x]\n",
    "\n",
    "mcs = np.where(qn_245 < 0,0,mcs)        \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "i_245 = nan_array(262144)\n",
    "\n",
    "dummy = 0\n",
    "\n",
    "for z_i in np.arange(0,64):\n",
    "#iterate over all z levels\n",
    "    \n",
    "    if len(np.where(np.isnan(i_245)==False)[0]) < 262144:   \n",
    "        #iterate over all z levels\n",
    "        \n",
    "        #create empty dummy arrays\n",
    "        i_0 = nan_array(262144)\n",
    "        \n",
    "        #collect indices where t, at given z, is less than 245K\n",
    "        indices = np.where(temp[0,z_i,:,:].data.flatten() < 245)\n",
    "        \n",
    "        \n",
    "        if len(indices[0]) > 0:\n",
    "            \n",
    "            \n",
    "            i_0[indices] = indices\n",
    "        \n",
    "            i_245_new = i_0\n",
    "            \n",
    "            dummy += 1  \n",
    "            \n",
    "            \n",
    "            if dummy == 1:\n",
    "                \n",
    "            \n",
    "                i_245 = i_245_new\n",
    "            \n",
    "            \n",
    "                zi_0 = nan_array(262144)\n",
    "            \n",
    "                zi_0[indices] = z_i\n",
    "            \n",
    "                zi_245 = zi_0\n",
    "            \n",
    "            else:\n",
    "                            \n",
    "                i_diff = np.where(i_245 != i_245_new)\n",
    "\n",
    "                i_245[i_diff] = i_245_new[i_diff]\n",
    "            \n",
    "                zi_245[i_diff] = z_i\n",
    "                \n",
    "print(i_245)\n",
    "                \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def search_z(four_d_arr,time_index,less_than_val):\n",
    "    \n",
    "    xy_size = np.size(four_d_arr[time_index,0,:,:])\n",
    "    \n",
    "    i_arr = nan_array(xy_size)\n",
    "\n",
    "    dummy = 0\n",
    "\n",
    "    for z_i in np.arange(0,len(four_d_arr[time_index,:,0,0])):\n",
    "    #iterate over all z levels\n",
    "    \n",
    "    \n",
    "        if len(np.where(np.isnan(i_arr)==False)[0]) < xy_size:   \n",
    "        \n",
    "            #create empty dummy arrays\n",
    "            i_0 = nan_array(xy_size)\n",
    "        \n",
    "            #collect indices where the array, at given z, is less than the prescribed value\n",
    "            indices = np.where(four_d_arr[time_index,z_i,:,:].data.flatten() < less_than_val)\n",
    "        \n",
    "        \n",
    "            if len(indices[0]) > 0:\n",
    "            \n",
    "            \n",
    "                i_0[indices] = indices\n",
    "        \n",
    "                i_arr_new = i_0\n",
    "            \n",
    "                dummy += 1  \n",
    "            \n",
    "            \n",
    "                if dummy == 1:\n",
    "                \n",
    "            \n",
    "                    i_arr = i_arr_new\n",
    "            \n",
    "            \n",
    "                    zi_0 = nan_array(xy_size)\n",
    "            \n",
    "                    zi_0[indices] = z_i\n",
    "            \n",
    "                    zi_arr = zi_0\n",
    "            \n",
    "                else:\n",
    "                            \n",
    "                    i_diff = np.where(i_arr != i_arr_new)\n",
    "\n",
    "                    i_arr[i_diff] = i_arr_new[i_diff]\n",
    "            \n",
    "                    zi_arr[i_diff] = z_i\n",
    "                \n",
    "    zi_arr = zi_arr.astype('int32') \n",
    "    \n",
    "    xyz_indices = np.array([],dtype=int)\n",
    "    \n",
    "    for i,value in enumerate(zi_arr):\n",
    "    \n",
    "        xyz_indices = np.append(xyz_indices,value*(xy_size) + i)\n",
    "    \n",
    "    return xyz_indices\n",
    "                \n",
    "              \n",
    "full_indices = search_z(temp,0,245)\n",
    "\n",
    "print(full_indices)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "#convert Boolean array into integers\n",
    "mcs = mcs.astype('int32')\n",
    "\n",
    "#call the labels and number of clusters using ndimage\n",
    "labels,nb = ndimage.label(mcs)\n",
    "\n",
    "#plot the labelled clusters\n",
    "fig,ax = plt.subplots(figsize = (64*4,4))\n",
    "ax.imshow(labels)\n",
    "plt.show()\n",
    "\n",
    "#find the sizes of each object, in terms of pixels \n",
    "pixelsizes = ndimage.sum(mcs,labels,range(nb+1))\n",
    "\n",
    "#create a boolean array for whether the objects \n",
    "#are larger than a given threshold\n",
    "mask_size = pixelsizes < 310\n",
    "\n",
    "#indices of pixels to be removed are produced\n",
    "remove_pixel = mask_size[labels]\n",
    "\n",
    "#indices and thus objects are removed based on object size\n",
    "labels[remove_pixel] = 0\n",
    "\n",
    "#second figure produced\n",
    "fig,ax = plt.subplots(figsize = (64*4,4))\n",
    "\n",
    "ax.imshow(labels)\n",
    "\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#qn_245_3 = search_z(temp,0,245,qn)\n",
    "\n",
    "\"\"\"\n",
    "#set mcs values < 0 to 0 in boolean array\n",
    "mcs_1[np.where(qn_245_3<0)] = 0\n",
    "\n",
    "#colormesh of qn_245_2\n",
    "fig,ax = plt.subplots(figsize = (64*4,4))\n",
    "\n",
    "#showing the qn_245 array\n",
    "mesh_qn = ax.pcolormesh(qn_245_3,cmap='Greys')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(qn_245_2[np.where(qn_245_2!=0)],qn_245_2[np.where(qn_245_3!=0)])\n",
    "\"\"\"\n",
    "\n",
    "#qn_245_check = qn_245_3 - qn_245_2\n",
    "\n",
    "#colormesh of qn_245_2\n",
    "#fig,ax = plt.subplots(figsize = (64*4,4))\n",
    "\n",
    "#showing the qn_245 array\n",
    "#mesh_qn = ax.pcolormesh(qn_245_check,cmap='Greys')\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "\"\"\"\n",
    "def search_z(four_d_arr,time_index,less_than_val,four_d_arr_2):\n",
    "    #function that searches an array four_d_arr for the z values at each x,y value, \n",
    "    #where the data is below a given value {{ could be edited to be another condition???}}\n",
    "    \n",
    "    #four_d_arr [time,z,y,x], array for inspection\n",
    "    #time_index to take values at \n",
    "    #less_than_val is the value which elements need to be below to be taken\n",
    "    #four_d_arr_2 is the second array to then extract values from using the four_d_arr indices of interest\n",
    "    \n",
    "    #take shape of input arr\n",
    "    xy_shape = np.shape(four_d_arr[time_index,0,:,:])\n",
    "    \n",
    "    #calculate total size\n",
    "    xy_size = xy_shape[0]*xy_shape[1]\n",
    "    \n",
    "    #initialise empty data array for later outputting\n",
    "    arr_2d = np.zeros(xy_size)\n",
    "    \n",
    "    #array of indices, used internally\n",
    "    i_arr = nan_array(xy_size)\n",
    "\n",
    "    #used to set off first instance where values are found\n",
    "    dummy = 0\n",
    "\n",
    "    for z_i in np.arange(0,len(four_d_arr[time_index,:,0,0])):\n",
    "    #iterate over all z levels\n",
    "    \n",
    "        #only keep searching when not all values in 2d xy array are filled\n",
    "        if len(np.where(np.isnan(i_arr)==False)[0]) < xy_size:   \n",
    "        \n",
    "            #create empty dummy arrays\n",
    "            i_0 = nan_array(xy_size)\n",
    "        \n",
    "            #collect indices where the array, at given z, is less than the prescribed value\n",
    "            indices = np.where(four_d_arr[time_index,z_i,:,:].data.flatten() < less_than_val)\n",
    "        \n",
    "            if len(indices[0]) > 0:\n",
    "\n",
    "                #empty index array filled with indices\n",
    "                i_0[indices] = indices\n",
    "        \n",
    "                i_arr_new = i_0\n",
    "                \n",
    "                dummy += 1  \n",
    "            \n",
    "                if dummy == 1:\n",
    "                \n",
    "                    i_arr = i_arr_new\n",
    "                \n",
    "                    #arr_2d is added to by corresponding values in secondary array, at correct z and xy\n",
    "                    arr_2d[indices] = (four_d_arr_2[time_index,z_i].data.flatten())[indices]\n",
    "            \n",
    "                else:\n",
    "                    \n",
    "                    #check to only add where existing values are not present\n",
    "                    #this means only values taken from first found z levels are used\n",
    "                    i_diff = np.where(i_arr != i_arr_new)\n",
    "\n",
    "                    arr_2d[i_diff] = (four_d_arr_2[time_index,z_i].data.flatten())[i_diff]\n",
    "                    \n",
    "    #reshape to be same as original xy array\n",
    "    arr_2d = np.reshape(arr_2d,xy_shape)\n",
    "    \n",
    "    return arr_2d\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def integrate_z(arr,z_list):\n",
    "    \n",
    "    arr_flat = list(map(lambda arr: arr.flatten(),arr))\n",
    "    \n",
    "    #print(arr_flat)\n",
    "    print(arr_flat[:][0])\n",
    "    \n",
    "    length = len(arr_flat[0])\n",
    "    \n",
    "    arr_out = np.empty(length)\n",
    "    \n",
    "    for i in np.arange(0,length):\n",
    "        \n",
    "    \n",
    "        arr_out[i] = np.trapz(x=z_list,y=arr_flat[:][0][i])\n",
    "        \n",
    "    return arr_out\n",
    "\n",
    "integrate_z(buoy,z_2.data)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "index_550,p_550 = find_less(press_2,550,1)\n",
    "\n",
    "qv_550 = qv_2[0,index_550].data\n",
    "\n",
    "qv_550 = np.where(mcs==1,qv_550,np.NaN)\n",
    "\n",
    "#print(len(np.where(np.isnan(qv_550)==False)[0]))\n",
    "\n",
    "t_550 = temp_2[0,index_550].data\n",
    "\n",
    "t_550 = np.where(mcs==1,t_550,np.NaN)\n",
    "\n",
    "sd_550 = sd(qv_550,p_550,t_550)\n",
    "\n",
    "print(qv_550)\n",
    "print(t_550)\n",
    "#print(sd_550)\n",
    "\n",
    "#print(len(np.where(np.isnan(sd_550)==False)[0]))\n",
    "print('################################')\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "fig,ax = plt.subplots(figsize=(64*4,4))\n",
    "\n",
    "ax.imshow(psurf)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(64*4,4))\n",
    "\n",
    "ax.imshow(clusters_1[0],cmap='Greys')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "psurf = np.where(clusters_1[0]==0,0,psurf)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(64*4,4))\n",
    "\n",
    "ax.imshow(psurf)\n",
    "\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "fig,ax = plt.subplots(figsize=(64*4,4))\n",
    "\n",
    "ax.imshow(cape)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(64*4,4))\n",
    "\n",
    "ax.imshow(clusters_1[0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "cape = np.where(clusters_1[0]==0,0,cape)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(64*4,4))\n",
    "\n",
    "ax.imshow(cape)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "fig,ax = plt.subplots(figsize = (64*4,4))\n",
    "ax.imshow(mcs_0,cmap='Greys')\n",
    "plt.show()\n",
    "\n",
    "#cluster function called on mcs, with size corresponding to \n",
    "#equivalent radius of 90km\n",
    "clusters_0,clusters_1 = clusters(mcs_0,310)\n",
    "\n",
    "#plot the labelled clusters\n",
    "fig,ax = plt.subplots(figsize = (64*4,4))\n",
    "ax.imshow(clusters_0[0])\n",
    "plt.show()\n",
    "\n",
    "#second figure produced\n",
    "fig,ax = plt.subplots(figsize = (64*4,4))\n",
    "ax.imshow(clusters_1[0])\n",
    "plt.show()\n",
    "\n",
    "print(np.unique(clusters_1[0]))\n",
    "print(clusters_1[1])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#fig,axes = plt.subplots(3,1,figsize=(64*4,4))\n",
    "\n",
    "#im_1 = axes[0].imshow(mcs_0,cmap='Greys')\n",
    "\n",
    "#im_2 = axes[1].imshow(buoy,vmin=np.min(buoy),vmax=np.max(buoy))\n",
    "\n",
    "#im_3 = axes[2].imshow(buoy_1,vmin=np.min(buoy),vmax=np.max(buoy))\n",
    "\n",
    "#cb_ax = fig.add_axes([0.628, 0.05, 0.0025, 0.9])\n",
    "#fig.colorbar(im_2, ax=axes.ravel().tolist(),pad=0.001)\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "#call the sd generating routine for 3 pressure levels of interest\n",
    "#sd_550 = sd_plev(press_2,550,qv_2,temp_2)\n",
    "#sd_700 = sd_plev(press_2,700,qv_2,temp_2)\n",
    "#sd_850 = sd_plev(press_2,850,qv_2,temp_2)\n",
    "\n",
    "#collate arrays and reshape to iteratable form\n",
    "#sd_arrs = np.reshape([sd_550,sd_700,sd_850],(3,64,4096))\n",
    "#print(sd_arrs)\n",
    "\n",
    "#avergae over every x,y site\n",
    "#sd_avg = np.mean(sd_arrs,0)\n",
    "\n",
    "#sd_collocated = field_plots(clusters_1[0],sd_avg,['Saturation Deficit','N'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = np.indices((100,100))\n",
    "\n",
    "bw = np.sin(2*np.pi*x) \n",
    "\n",
    "bw=plt.pcolormesh(x,cmap='Greys')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels,nb = ndimage.label(bw)\n",
    "\n",
    "print(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x, y = np.indices((100, 100))\n",
    "sig = np.sin(2*np.pi*x/50.) * np.sin(2*np.pi*y/50.) * (1+x*y/50.**2)**2\n",
    "mask = sig > 1.5\n",
    "\n",
    "plt.figure(figsize=(7, 3.5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sig)\n",
    "plt.axis('off')\n",
    "plt.title('sig')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mask, cmap=plt.cm.gray)\n",
    "plt.axis('off')\n",
    "plt.title('mask')\n",
    "plt.subplots_adjust(wspace=.05, left=.01, bottom=.01, right=.99, top=.9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "labels, nb = ndimage.label(mask)\n",
    "\n",
    "plt.figure(figsize=(3.5, 3.5))\n",
    "plt.imshow(labels)\n",
    "plt.title('label')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=.05, left=.01, bottom=.01, right=.99, top=.9)\n",
    "plt.show()\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4,4,5,6,7,7]\n",
    "\n",
    "y = x\n",
    "\n",
    "y = y[:4]\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(0,10,size=(5,3))\n",
    "\n",
    "print(a)\n",
    "\n",
    "print(a[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp_2[0],qv_2[0],qn_2[0])\n",
    "print(tenv,qvenv,qnenv)\n",
    "\n",
    "buoyancy = buoy_function(temp_2[0].data,qv_2[0].data,qn_2[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (using the module python/3.5.2)",
   "language": "python",
   "name": "python_3.5.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
